{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "978b9e49-aab0-429e-8292-22db6e02d1b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm, trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from models import build_model\n",
    "from data_distributor import *\n",
    "from dataloaders import *\n",
    "import aggregation_rules\n",
    "from aggregation_rules import Aggregator\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed_value = 1\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_disk( path):\n",
    "    '''\n",
    "    Load the gradient_dataset from disk as a pickle file (name.pkl)\n",
    "    '''\n",
    "    with open(path, 'rb') as f:\n",
    "        gradient_dataset = pickle.load(f)\n",
    "    # input_grads = gradient_dataset['input'][0].reshape(-1, 1)\n",
    "    # target_grads = gradient_dataset['target'][0].reshape(-1, 1)\n",
    "    # input_grads = np.array([item for sublist in gradient_dataset['input'] for item in sublist])\n",
    "    # target_grads = np.array([item for sublist in gradient_dataset['target'] for item in sublist])\n",
    "    input_grads = np.array(gradient_dataset['input'])\n",
    "    target_grads = np.array(gradient_dataset['target'])\n",
    "    return input_grads, target_grads\n",
    "\n",
    "def sample_data(datalist, sample_size=None, percent=None):\n",
    "    '''\n",
    "    Sample the data from the datalist\n",
    "    args:\n",
    "        - datalist: [data1, data2, ...]\n",
    "        - data1: numpy array of shape (num_samples, num_features)\n",
    "    '''\n",
    "    if sample_size is None and percent is None:\n",
    "        raise ValueError(\"Either sample_size or percent must be specified.\")\n",
    "    elif sample_size is None and percent is not None:\n",
    "        # calculate the sample size\n",
    "        sample_size = int(datalist[0].shape[0] * percent)\n",
    "        # print(sample_size)\n",
    "\n",
    "    indices = np.random.choice(datalist[0].shape[0], sample_size, replace=False)\n",
    "    return [data[indices] for data in datalist]\n",
    "\n",
    "def data_standardization(inputs, targets):\n",
    "    '''\n",
    "    Standardize the data\n",
    "    args:\n",
    "        - datalist: [data1, data2, ...]\n",
    "        - data1: numpy array of shape (num_samples, num_features)\n",
    "    '''\n",
    "    # standardize the dataset\n",
    "    input_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "    # input_scaler = Normalizer()\n",
    "\n",
    "    input_scaler.fit(inputs)\n",
    "    inputs_std = input_scaler.transform(inputs)\n",
    "\n",
    "    target_scaler.fit(targets)\n",
    "    targets_std = target_scaler.transform(targets)\n",
    "\n",
    "    # return inputs_std, targets, input_scaler#, target_scaler\n",
    "    return inputs_std, targets_std, input_scaler, target_scaler\n",
    "\n",
    "def data_destandardization(data, scaler):\n",
    "    '''\n",
    "    De-standardize the data\n",
    "    args:\n",
    "        - data: numpy array of shape (num_samples, num_features)\n",
    "        - scaler: sklearn.preprocessing.StandardScaler\n",
    "    '''\n",
    "    return scaler.inverse_transform(data.reshape(-1, 1))\n",
    "\n",
    "def inference(input_grads, model, input_scaler, target_scaler):\n",
    "    '''\n",
    "    Inference the model\n",
    "    args:\n",
    "        - grad_list: list of gradients\n",
    "        - model: model\n",
    "        - input_scaler: sklearn.preprocessing.StandardScaler\n",
    "        - target_scaler: sklearn.preprocessing.StandardScaler\n",
    "    '''\n",
    "    if len(input_grads.shape) == 1:\n",
    "        input_grads = input_grads.reshape(-1, 1)\n",
    "\n",
    "    # standardize the data\n",
    "    input_grads_std = input_scaler.transform(input_grads)\n",
    "    # inference\n",
    "    pred_std = model.predict(input_grads_std)\n",
    "    # de-standardize the data\n",
    "    # pred_grads = pred_std.reshape(-1, 1)\n",
    "    pred_grads = target_scaler.inverse_transform(pred_std.reshape(-1, 1))\n",
    "    return pred_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data_path, data_coverage):\n",
    "    # Load the dataset\n",
    "    input_grads, target_grads = load_dataset_from_disk(data_path)\n",
    "\n",
    "    X_train, y_train = input_grads[0].reshape(-1, 1), target_grads[0].reshape(-1, 1)\n",
    "    X_test, y_test = input_grads[1:], target_grads[1:]\n",
    "\n",
    "    X_train, y_train = sample_data([X_train, y_train], percent=data_coverage)\n",
    "    # print(X_train.shape, y_train.shape)\n",
    "\n",
    "    # Splitting the data into training and testing sets\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(input_grads, target_grads, test_size=0.3, random_state=seed_value)\n",
    "\n",
    "    # standardize training data\n",
    "    X_train_std, y_train_std, in_scaler, tar_scaler = data_standardization(X_train, y_train)\n",
    "    # X_train_std, y_train_std, in_scaler = data_standardization(X_train, y_train)\n",
    "    return X_train_std, y_train_std, X_test, y_test, in_scaler, tar_scaler\n",
    "\n",
    "def test(model, X_test, y_test, in_scaler, tar_scaler):\n",
    "    mse_list = []\n",
    "    mape_list = []\n",
    "    for i, v in enumerate(X_test):\n",
    "        gt = y_test[i].reshape(-1, 1)\n",
    "        y_pred = inference(X_test[i], model, in_scaler, tar_scaler)\n",
    "        mse = mean_squared_error(gt, y_pred)\n",
    "        mape = mean_absolute_percentage_error(gt+1, y_pred+1)\n",
    "        mse_list.append(mse)\n",
    "        mape_list.append(mape)\n",
    "    return np.mean(mse_list), np.mean(mape_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_std, y_train_std, X_test, y_test, in_scaler, tar_scaler = data_prep(data_path=\"../data/grad_datasets/determ/cifar100_cusCNN.pkl\", data_coverage=0.1)\n",
    "\n",
    "# metric_dict = {'data_coverage': 0.1, 'fit_time': 0, 'mse': 0, 'mape': 0}\n",
    "# model = LinearRegression()\n",
    "# # model = MLPRegressor(hidden_layer_sizes=(100,), max_iter=100)\n",
    "\n",
    "# # time the training\n",
    "# start_time = time.time()\n",
    "# model.fit(X_train_std, y_train_std)\n",
    "# metric_dict['fit_time'] = time.time() - start_time\n",
    "# metric_dict['mse'], metric_dict['mape'] = test(model, X_test, y_test, in_scaler, tar_scaler)\n",
    "\n",
    "# pprint(metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path=\"../data/grad_datasets/determ/cifar100_cusCNN.pkl\"\n",
    "\n",
    "# noise_type = data_path.split('/')[-2]\n",
    "# dataset, model = data_path[:-4].split('/')[-1].split('_')\n",
    "# noise_type, dataset, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/grad_datasets/determ/mnist_cusMLP.pkl\n",
      "../data/grad_datasets/determ/cifar10_cusCNN.pkl\n",
      "../data/grad_datasets/determ/cifar10_resnet18.pkl\n",
      "../data/grad_datasets/determ/cifar100_cusCNN.pkl\n",
      "../data/grad_datasets/determ/cifar100_resnet18.pkl\n",
      "../data/grad_datasets/randm_proc/mnist_cusMLP.pkl\n",
      "../data/grad_datasets/randm_proc/cifar10_cusCNN.pkl\n",
      "../data/grad_datasets/randm_proc/cifar10_resnet18.pkl\n",
      "../data/grad_datasets/randm_proc/cifar100_cusCNN.pkl\n",
      "../data/grad_datasets/randm_proc/cifar100_resnet18.pkl\n",
      "../data/grad_datasets/randm/cifar10_resnet18.pkl\n",
      "../data/grad_datasets/randm/cifar100_cusCNN.pkl\n",
      "../data/grad_datasets/randm/cifar100_resnet18.pkl\n",
      "../data/grad_datasets/randm/mnist_cusMLP.pkl\n",
      "../data/grad_datasets/randm/cifar10_cusCNN.pkl\n"
     ]
    }
   ],
   "source": [
    "data_root = \"../data/grad_datasets/\"\n",
    "data_paths = []\n",
    "for dir in os.listdir(data_root):\n",
    "    n_type = os.path.join(data_root, dir)\n",
    "    for data in os.listdir(n_type):\n",
    "        full_path = os.path.join(n_type, data)\n",
    "        print(full_path)\n",
    "        data_paths.append(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/grad_datasets/determ/mnist_cusMLP.pkl\n",
      "../data/grad_datasets/determ/cifar10_cusCNN.pkl\n",
      "../data/grad_datasets/determ/cifar10_resnet18.pkl\n",
      "../data/grad_datasets/determ/cifar100_cusCNN.pkl\n",
      "../data/grad_datasets/determ/cifar100_resnet18.pkl\n",
      "../data/grad_datasets/randm_proc/mnist_cusMLP.pkl\n",
      "../data/grad_datasets/randm_proc/cifar10_cusCNN.pkl\n",
      "../data/grad_datasets/randm_proc/cifar10_resnet18.pkl\n",
      "../data/grad_datasets/randm_proc/cifar100_cusCNN.pkl\n",
      "../data/grad_datasets/randm_proc/cifar100_resnet18.pkl\n",
      "../data/grad_datasets/randm/cifar10_resnet18.pkl\n",
      "../data/grad_datasets/randm/cifar100_cusCNN.pkl\n",
      "../data/grad_datasets/randm/cifar100_resnet18.pkl\n",
      "../data/grad_datasets/randm/mnist_cusMLP.pkl\n",
      "../data/grad_datasets/randm/cifar10_cusCNN.pkl\n"
     ]
    }
   ],
   "source": [
    "data_coverage_list = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "eval_list = []\n",
    "for p in data_paths:\n",
    "    print(p)\n",
    "    for dc in data_coverage_list:\n",
    "        noise_type = p.split('/')[-2]\n",
    "        dataset, model = p[:-4].split('/')[-1].split('_')\n",
    "        metric_dict = {'correction_model':'LR', 'dataset': dataset, 'model': model, 'noise_type': noise_type,\n",
    "                       'data_coverage': dc, 'fit_time (s)': 0, 'mse': 0, 'mape': 0, 'sample_size': 0}\n",
    "\n",
    "        X_train_std, y_train_std, X_test, y_test, in_scaler, tar_scaler = data_prep(data_path=p, data_coverage=dc)\n",
    "        metric_dict['sample_size'] = X_train_std.shape[0]\n",
    "        model = LinearRegression()\n",
    "        # model = MLPRegressor(hidden_layer_sizes=(100,), max_iter=100)\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train_std, y_train_std)\n",
    "        metric_dict['fit_time (s)'] = time.time() - start_time\n",
    "        metric_dict['mse'], metric_dict['mape'] = test(model, X_test, y_test, in_scaler, tar_scaler)\n",
    "        eval_list.append(metric_dict)\n",
    "        # pprint(metric_dict)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/grad_datasets/determ/mnist_cusMLP.pkl\n",
      "../data/grad_datasets/determ/cifar10_cusCNN.pkl\n",
      "../data/grad_datasets/determ/cifar10_resnet18.pkl\n",
      "../data/grad_datasets/determ/cifar100_cusCNN.pkl\n",
      "../data/grad_datasets/determ/cifar100_resnet18.pkl\n",
      "../data/grad_datasets/randm_proc/mnist_cusMLP.pkl\n",
      "../data/grad_datasets/randm_proc/cifar10_cusCNN.pkl\n",
      "../data/grad_datasets/randm_proc/cifar10_resnet18.pkl\n",
      "../data/grad_datasets/randm_proc/cifar100_cusCNN.pkl\n",
      "../data/grad_datasets/randm_proc/cifar100_resnet18.pkl\n",
      "../data/grad_datasets/randm/cifar10_resnet18.pkl\n",
      "../data/grad_datasets/randm/cifar100_cusCNN.pkl\n",
      "../data/grad_datasets/randm/cifar100_resnet18.pkl\n",
      "../data/grad_datasets/randm/mnist_cusMLP.pkl\n",
      "../data/grad_datasets/randm/cifar10_cusCNN.pkl\n"
     ]
    }
   ],
   "source": [
    "for p in data_paths:\n",
    "    print(p)\n",
    "    for dc in data_coverage_list:\n",
    "        noise_type = p.split('/')[-2]\n",
    "        dataset, model = p[:-4].split('/')[-1].split('_')\n",
    "        metric_dict = {'correction_model':'MLP10_10_200iter', 'dataset': dataset, 'model': model, 'noise_type': noise_type,\n",
    "                       'data_coverage': dc, 'fit_time (s)': 0, 'mse': 0, 'mape': 0, 'sample_size': 0}\n",
    "\n",
    "        X_train_std, y_train_std, X_test, y_test, in_scaler, tar_scaler = data_prep(data_path=p, data_coverage=dc)\n",
    "        metric_dict['sample_size'] = X_train_std.shape[0]\n",
    "        # model = LinearRegression()\n",
    "        model = MLPRegressor(hidden_layer_sizes=(10,10,), max_iter=200)\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train_std, y_train_std)\n",
    "        metric_dict['fit_time (s)'] = time.time() - start_time\n",
    "        metric_dict['mse'], metric_dict['mape'] = test(model, X_test, y_test, in_scaler, tar_scaler)\n",
    "        eval_list.append(metric_dict)\n",
    "        # pprint(metric_dict)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correction_model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>noise_type</th>\n",
       "      <th>data_coverage</th>\n",
       "      <th>fit_time (s)</th>\n",
       "      <th>mse</th>\n",
       "      <th>mape</th>\n",
       "      <th>sample_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>mnist</td>\n",
       "      <td>cusMLP</td>\n",
       "      <td>determ</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.013259</td>\n",
       "      <td>1.788503e-19</td>\n",
       "      <td>2.633291e-10</td>\n",
       "      <td>46887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR</td>\n",
       "      <td>mnist</td>\n",
       "      <td>cusMLP</td>\n",
       "      <td>determ</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>2.800072e-18</td>\n",
       "      <td>1.163307e-09</td>\n",
       "      <td>140662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>mnist</td>\n",
       "      <td>cusMLP</td>\n",
       "      <td>determ</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>1.464369e-17</td>\n",
       "      <td>3.114732e-09</td>\n",
       "      <td>234437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR</td>\n",
       "      <td>mnist</td>\n",
       "      <td>cusMLP</td>\n",
       "      <td>determ</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.003462</td>\n",
       "      <td>1.887340e-19</td>\n",
       "      <td>3.024214e-10</td>\n",
       "      <td>328211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR</td>\n",
       "      <td>mnist</td>\n",
       "      <td>cusMLP</td>\n",
       "      <td>determ</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.004339</td>\n",
       "      <td>5.195344e-18</td>\n",
       "      <td>1.557856e-09</td>\n",
       "      <td>421986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>MLP10_10_200iter</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>cusCNN</td>\n",
       "      <td>randm</td>\n",
       "      <td>0.3</td>\n",
       "      <td>14.441177</td>\n",
       "      <td>2.295606e-04</td>\n",
       "      <td>9.494296e-04</td>\n",
       "      <td>621870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>MLP10_10_200iter</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>cusCNN</td>\n",
       "      <td>randm</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25.368040</td>\n",
       "      <td>2.295487e-04</td>\n",
       "      <td>9.354309e-04</td>\n",
       "      <td>1036451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>MLP10_10_200iter</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>cusCNN</td>\n",
       "      <td>randm</td>\n",
       "      <td>0.7</td>\n",
       "      <td>37.491253</td>\n",
       "      <td>2.295606e-04</td>\n",
       "      <td>9.495156e-04</td>\n",
       "      <td>1451031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>MLP10_10_200iter</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>cusCNN</td>\n",
       "      <td>randm</td>\n",
       "      <td>0.9</td>\n",
       "      <td>47.595510</td>\n",
       "      <td>2.295323e-04</td>\n",
       "      <td>9.167629e-04</td>\n",
       "      <td>1865611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>MLP10_10_200iter</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>cusCNN</td>\n",
       "      <td>randm</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.282421</td>\n",
       "      <td>2.294876e-04</td>\n",
       "      <td>9.043891e-04</td>\n",
       "      <td>2072902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     correction_model  dataset   model noise_type  data_coverage  \\\n",
       "0                  LR    mnist  cusMLP     determ            0.1   \n",
       "1                  LR    mnist  cusMLP     determ            0.3   \n",
       "2                  LR    mnist  cusMLP     determ            0.5   \n",
       "3                  LR    mnist  cusMLP     determ            0.7   \n",
       "4                  LR    mnist  cusMLP     determ            0.9   \n",
       "..                ...      ...     ...        ...            ...   \n",
       "175  MLP10_10_200iter  cifar10  cusCNN      randm            0.3   \n",
       "176  MLP10_10_200iter  cifar10  cusCNN      randm            0.5   \n",
       "177  MLP10_10_200iter  cifar10  cusCNN      randm            0.7   \n",
       "178  MLP10_10_200iter  cifar10  cusCNN      randm            0.9   \n",
       "179  MLP10_10_200iter  cifar10  cusCNN      randm            1.0   \n",
       "\n",
       "     fit_time (s)           mse          mape  sample_size  \n",
       "0        0.013259  1.788503e-19  2.633291e-10        46887  \n",
       "1        0.001928  2.800072e-18  1.163307e-09       140662  \n",
       "2        0.002637  1.464369e-17  3.114732e-09       234437  \n",
       "3        0.003462  1.887340e-19  3.024214e-10       328211  \n",
       "4        0.004339  5.195344e-18  1.557856e-09       421986  \n",
       "..            ...           ...           ...          ...  \n",
       "175     14.441177  2.295606e-04  9.494296e-04       621870  \n",
       "176     25.368040  2.295487e-04  9.354309e-04      1036451  \n",
       "177     37.491253  2.295606e-04  9.495156e-04      1451031  \n",
       "178     47.595510  2.295323e-04  9.167629e-04      1865611  \n",
       "179     52.282421  2.294876e-04  9.043891e-04      2072902  \n",
       "\n",
       "[180 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(eval_list)\n",
    "\n",
    "df = pd.DataFrame(eval_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('../data/correction_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_MAPELoss(y_pred, y_true):\n",
    "    mape = torch.mean(torch.abs((y_true - y_pred) / y_true))\n",
    "    mse = torch.mean(torch.pow(y_true - y_pred, 2))\n",
    "    loss = mse\n",
    "    if not torch.isnan(mape) and mape<0.05:\n",
    "        loss = mse + mape\n",
    "    return loss\n",
    "\n",
    "class CorModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CorModel, self).__init__()\n",
    "        self.fc = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def fit(self, X_train_std, y_train_std, epochs=10, verbose=False):\n",
    "        self.train()\n",
    "        x_data, y_data = torch.tensor(X_train_std, dtype=torch.float32).view(-1, 1), torch.tensor(y_train_std, dtype=torch.float32).view(-1, 1)\n",
    "        dataset = TensorDataset(x_data, y_data)\n",
    "        data_loader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = MSE_MAPELoss\n",
    "        for ep in range(epochs):\n",
    "            with tqdm(data_loader) as tepoch:\n",
    "                tepoch.set_description(f\"EP {ep+1}/{epochs}\")\n",
    "                for x, y in tepoch:\n",
    "                    # print(x.shape, y.shape)\n",
    "                    optimizer.zero_grad()\n",
    "                    y_pred = model(x)\n",
    "                    loss = criterion(y_pred, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    tepoch.set_postfix({'train_loss': loss.item()})\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x_data = torch.tensor(x, dtype=torch.float32).view(-1, 1)\n",
    "        self.eval()\n",
    "        return self(x_data).detach().numpy()\n",
    "    \n",
    "\n",
    "model.fit(X_train_std, y_train_std, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for i, v in enumerate(X_test):\n",
    "    gt = y_test[i].reshape(-1, 1)\n",
    "    # std input data\n",
    "    X_test_std = in_scaler.transform(X_test[i].reshape(-1, 1))\n",
    "    y_test_std = tar_scaler.transform(y_test[i].reshape(-1, 1))\n",
    "    # print(X_test_std.shape)\n",
    "    x_data = torch.tensor(X_test_std, dtype=torch.float32, requires_grad=False)\n",
    "    y_data = torch.tensor(y_test_std, dtype=torch.float32, requires_grad=False)\n",
    "    # print(x_data.shape)\n",
    "\n",
    "    # make test data loader\n",
    "    dataset = TensorDataset(x_data, y_data)\n",
    "    data_loader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    # inference\n",
    "    y_pred = []\n",
    "    with tqdm(data_loader) as tepoch:\n",
    "        for x, y in tepoch:\n",
    "            batch_preds = model(x)\n",
    "            y_pred.extend(batch_preds.detach().numpy().tolist())\n",
    "\n",
    "    break\n",
    "    mse = mean_squared_error(gt, y_pred)\n",
    "    print(f\"MSE : {mse}\")\n",
    "    mape = mean_absolute_percentage_error(gt, y_pred)\n",
    "    print(f\"MAPE: {mape}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_destd = tar_scaler.inverse_transform(np.array(y_pred).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(gt, y_pred)\n",
    "print(f\"MSE : {mse}\")\n",
    "mape = mean_absolute_percentage_error(gt+1, y_pred_destd+1)\n",
    "print(f\"MAPE: {mape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_destd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
