{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "978b9e49-aab0-429e-8292-22db6e02d1b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from multiprocessing import Process, Manager, Pool\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from models import build_model\n",
    "from data_distributor import *\n",
    "from dataloaders import *\n",
    "import aggregation_rules\n",
    "from aggregation_rules import Aggregator\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed_value = 1\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bfc9d09-e311-4821-9706-2add1e12689e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Worker process\n",
    "def worker(idx, model, data, target, gradients_dict, loss_list, optimizer, criterion, faulty):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    # loss = nn.MSELoss()(output, target)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    if faulty:\n",
    "        # print(\"faulty worker\")\n",
    "        # add gaussian noise to gradients\n",
    "        gradients = [torch.randn_like(p.grad)*100 for p in model.parameters()]\n",
    "        # gradients = [p.grad+(torch.randn_like(p.grad)*10) for p in model.parameters()]\n",
    "        # gradients = [torch.ones_like(p.grad) * 100000 for p in model.parameters()]\n",
    "        # convert gradients to numpy array\n",
    "    else:\n",
    "        gradients = [p.grad.clone() for p in model.parameters()]\n",
    "    \n",
    "    # gradients_list.append(gradients)\n",
    "    gradients_dict[idx] = gradients\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "def parallel_worker_train(args):\n",
    "    # this is a helper function for parallelizing worker\n",
    "    idx, model, data, target, gradients_list, loss_list, optimizer, criterion, faulty = args\n",
    "    worker(idx, model, data, target, gradients_list, loss_list, optimizer, criterion, faulty)\n",
    "\n",
    "def setup_train_job(dataset, global_batch_size):\n",
    "    # Initialize global model and optimizer, and set up datalaoder\n",
    "    if dataset==\"dummy\":\n",
    "        test_loader = None\n",
    "        data_loader = dummy_dataloader(f=lambda x: x ** 3, num_samples=500)\n",
    "        model = build_model(arch=\"simplemodel\", class_number=1)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "        criterion = nn.MSELoss()\n",
    "    elif dataset.lower()==\"mnist\":\n",
    "        data_loader, test_loader = mnist_dataloader(global_batch_size)\n",
    "        model = build_model(arch=\"mlp\", class_number=10)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif dataset.lower()==\"cifar100\":\n",
    "        data_loader, test_loader = CIFAR100_dataloader(global_batch_size)\n",
    "        model = build_model(arch=\"SimpleCNN\", class_number=100)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        raise ValueError(\"Dataset not supported\")\n",
    "    \n",
    "    return data_loader, test_loader, model, optimizer, criterion\n",
    "    \n",
    "def inference(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data_batch, target_batch in data_loader:\n",
    "        output = model(data_batch)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        # print(\"pred\", predicted)\n",
    "        # print(\"gt\", target_batch)\n",
    "        total += target_batch.size(0)\n",
    "        correct += (predicted == target_batch).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3982220d-6442-4fdf-9301-988695d365c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of workers: 4\n",
      "Number of processes: 1\n"
     ]
    }
   ],
   "source": [
    "dataset = \"mnist\"\n",
    "n_epochs = 2\n",
    "num_sub_batches = 4\n",
    "global_batch_size = 64\n",
    "faulty_worker_idxs = [0]\n",
    "defense_method = None\n",
    "num_processes = 1\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# print numbers worker, and proc\n",
    "print(f'Number of workers: {num_sub_batches}')\n",
    "print(f'Number of processes: {num_processes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abfe7afa-ed74-469b-9cdd-8203fdca4aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init correction model for worker 0\n",
      "[CORRECTION] Training ...\n",
      "[CORRECTION] Training loss (MSE): 0.0002963462\n",
      "Epoch: 1, sub-batch: 1/938, avg sub-batch loss: 2.313\n",
      "validation accuracy: 0.1051\n"
     ]
    }
   ],
   "source": [
    "# multiprocessing manager, and tensorboard writer\n",
    "manager = Manager()\n",
    "\n",
    "# setup train job\n",
    "data_loader, test_loader, global_model, optimizer, criterion = setup_train_job(dataset, global_batch_size)\n",
    "global_model.to(device)\n",
    "\n",
    "correct_args = {\n",
    "        'correct': True,\n",
    "        'cmodel': \"linear\",\n",
    "        'model': global_model\n",
    "    }\n",
    "aggregator = Aggregator(correct_args=correct_args)\n",
    "data_distributor = DDPDataDistributor(num_workers=num_sub_batches, faulty_worker_ids=faulty_worker_idxs)\n",
    "aggregator.update_faulty_worker_idxs(faulty_worker_idxs)\n",
    "\n",
    "training_iter = 0\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss_list = []\n",
    "    for iteration, (data_batch, target_batch) in enumerate(data_loader):\n",
    "        training_iter+=1\n",
    "        global_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        gradients_dict = manager.dict()\n",
    "        loss_list = manager.list()\n",
    "\n",
    "        # Divide the data_batch and target_batch into smaller batches // again simulating DDP\n",
    "        data_sub_batches, target_sub_batches, worker_batch_map = data_distributor.distribute(data_batch, target_batch)\n",
    "\n",
    "        faulty = False\n",
    "        args_list = []\n",
    "        for i, (data, target) in enumerate(zip(data_sub_batches, target_sub_batches)):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            if i in faulty_worker_idxs: faulty = True\n",
    "            else: faulty = False\n",
    "            args_list.append((i, global_model, data, target, gradients_dict, loss_list, optimizer, criterion, faulty))\n",
    "        with ThreadPool(processes=num_processes) as pool:\n",
    "            pool.map(parallel_worker_train, args_list)\n",
    "\n",
    "        # Assume faulty grad is detected here!!!\n",
    "        # faulty_worker_idxs: [0]\n",
    "\n",
    "        # gradients_list = list(gradients_dict.values())\n",
    "        # aggregated_gradients = aggregation_rules.average_grads(gradients_list)\n",
    "        aggregated_gradients = aggregator.aggregate(gradients_dict, worker_batch_map)\n",
    "\n",
    "        # Update global model\n",
    "        for p, agg_grad in zip(global_model.parameters(), aggregated_gradients):\n",
    "            p.grad = agg_grad\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_iter_loss = sum(loss_list) / len(loss_list)\n",
    "        epoch_loss_list.append(avg_iter_loss)\n",
    "        # print(f'Epoch: {epoch+1}, sub-batch: {iteration}, avg sub-batch loss: {round(avg_iter_loss, 3)}')\n",
    "        # print curr iter / total iter\n",
    "        print(f'Epoch: {epoch+1}, sub-batch: {iteration+1}/{len(data_loader)}, avg sub-batch loss: {round(avg_iter_loss, 3)}')\n",
    "\n",
    "        # validation per 10 iterations\n",
    "        if iteration % 10 == 0:\n",
    "            acc = inference(global_model, test_loader)\n",
    "            print(\"validation accuracy:\", acc)\n",
    "        \n",
    "        break\n",
    "        \n",
    "    break\n",
    "    # Compute and print the average epoch loss\n",
    "    avg_epoch_loss = sum(epoch_loss_list) / len(epoch_loss_list)\n",
    "    print(f'Epoch: {epoch+1} done, avg loss: {round(avg_epoch_loss, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c278061f-ef02-4455-a4ff-6139a33107f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 2, 3: 3}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_batch_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84ed83c-40dd-4778-94fd-61199ae44056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0cff0-7d02-4bd3-8769-b3e453b04697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f955b1-61db-49df-adc5-e2614bb8ae99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "class Monitor:\n",
    "    def __init__(self):\n",
    "        self.process = None\n",
    "        self.exit_event = multiprocessing.Event()\n",
    "        self.data_queue = multiprocessing.Queue()\n",
    "\n",
    "    def start(self):\n",
    "        self.process = multiprocessing.Process(target=self.run)\n",
    "        self.process.start()\n",
    "\n",
    "    def run(self):\n",
    "        while not self.exit_event.is_set():\n",
    "            if not self.data_queue.empty():\n",
    "                data = self.data_queue.get()\n",
    "                print(f\"Received data: {data}\")\n",
    "                # Process the data here\n",
    "            else:\n",
    "                time.sleep(1)  # Sleep if there's no data\n",
    "\n",
    "    def stop(self):\n",
    "        self.exit_event.set()\n",
    "        self.process.join()\n",
    "\n",
    "    def send_data(self, data):\n",
    "        self.data_queue.put(data)\n",
    "\n",
    "    def is_running(self):\n",
    "        return self.process is not None and self.process.is_alive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a3f33-2494-4ed3-858c-ebe846259146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Usage\n",
    "    monitor = Monitor()\n",
    "    monitor.start()\n",
    "\n",
    "    try:\n",
    "        # Main process does its work and sends data to the monitor\n",
    "        for i in range(5):\n",
    "            monitor.send_data(f\"Data {i}\")\n",
    "            time.sleep(1)\n",
    "    finally:\n",
    "        monitor.stop()\n",
    "        print(\"Monitor stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8c16f-bfc7-4755-adf7-3e5fb1fd4be0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd2c2a8-59e5-4b0e-9eef-bc53876966a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.08929456509440377\n",
      "Predicted output: [[0.53116393 0.52878067 0.43936241]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example data\n",
    "ndarray1 = np.random.rand(100, 3)  # Replace with your data\n",
    "ndarray2 = np.random.rand(100, 3)  # Replace with your data\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(ndarray1, ndarray2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Use the model to predict new values\n",
    "new_data = np.array([[0.5, 0.6, 0.7]])  # Example new data\n",
    "predicted = model.predict(new_data)\n",
    "print(f\"Predicted output: {predicted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bbd691e-d253-4fda-a84c-570e1da1effc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndarray1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da848eca-6332-45ce-b58d-25a2f5ba1285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
